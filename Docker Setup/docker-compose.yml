version: '3.8'

networks:
  default:
    external: true
    name: apache-stack_default

services:
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  airflow:
    build:
      context: .
      dockerfile: Dockerfile-airflow
    container_name: airflow
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - PYTHONPATH=/home/airflow/.local/lib/python3.10/site-packages
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__EMAIL__EMAIL_BACKEND=airflow.utils.email.send_email_smtp
      - AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
      - AIRFLOW__SMTP__SMTP_STARTTLS=True
      - AIRFLOW__SMTP__SMTP_SSL=False
      - AIRFLOW__SMTP__SMTP_PORT=587
      - AIRFLOW__SMTP__SMTP_MAIL_FROM=arin.rohega@gmail.com
      - AIRFLOW__SMTP__SMTP_USER=arin.rohega@gmail.com
      - AIRFLOW__SMTP__SMTP_PASSWORD=pmeoskolgupxelfe
    user: root
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./shared-data:/opt/airflow/shared-data
      - /var/run/docker.sock:/var/run/docker.sock
      - ./airflow/spark-scripts:/opt/airflow/spark-scripts  
      - ./airflow/spark-jars:/opt/airflow/spark-jars        
    ports:
      - "8082:8080"
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
        airflow webserver"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile-airflow
    container_name: airflow-scheduler
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - PYTHONPATH=/home/airflow/.local/lib/python3.10/site-packages
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      - AIRFLOW__EMAIL__EMAIL_BACKEND=airflow.utils.email.send_email_smtp
      - AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
      - AIRFLOW__SMTP__SMTP_STARTTLS=True
      - AIRFLOW__SMTP__SMTP_SSL=False
      - AIRFLOW__SMTP__SMTP_PORT=587
      - AIRFLOW__SMTP__SMTP_MAIL_FROM=arin.rohega@gmail.com
      - AIRFLOW__SMTP__SMTP_USER=arin.rohega@gmail.com
      - AIRFLOW__SMTP__SMTP_PASSWORD=pmeoskolgupxelfe
    user: root
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./shared-data:/opt/airflow/shared-data
      - /var/run/docker.sock:/var/run/docker.sock
      - ./airflow/spark-scripts:/opt/airflow/spark-scripts 
      - ./airflow/spark-jars:/opt/airflow/spark-jars        
    command: airflow scheduler

  nifi:
    image: apache/nifi:2.4.0
    container_name: nifi
    environment:
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=ctsBtRBKHRAx69EqUghvvgEvjnaLjFEB
      - NIFI_WEB_HTTPS_PORT=8443
      - NIFI_WEB_HTTP_PORT=8080
      - NIFI_CLUSTER_IS_NODE=false
      - NIFI_ZK_CONNECT_STRING=
      - NIFI_ELECTION_MAX_CANDIDATES=1
      - TZ=America/Mazatlan
    ports:
      - "8080:8080"
      - "8443:8443"
    volumes:
      - nifi_database_repository:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
      - nifi_content_repository:/opt/nifi/nifi-current/content_repository
      - nifi_provenance_repository:/opt/nifi/nifi-current/provenance_repository
      - nifi_state:/opt/nifi/nifi-current/state
      - nifi_logs:/opt/nifi/nifi-current/logs
      - ./shared-data:/opt/nifi/shared-data
      - ./nifi/lib:/opt/nifi/nifi-current/lib
      - ./nifi/certs/keystore.p12:/opt/nifi/nifi-current/conf/keystore.p12  
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/nifi/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Apache Spark Master
  spark:
    image: bitnami/spark:3.5.1
    container_name: spark
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8081:8080"  # Spark Web UI
      - "7077:7077"  # Spark Master Port
    volumes:
      - ./spark:/opt/bitnami/spark/work
      - ./shared-data:/opt/bitnami/spark/shared-data
      - ./spark/jars-ext:/opt/bitnami/spark/delta-jars
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Spark Worker
  spark-worker:
    image: bitnami/spark:3.5.1
    container_name: spark-worker
    depends_on:
      - spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - ./spark:/opt/bitnami/spark/work
      - ./shared-data:/opt/bitnami/spark/shared-data
      - ./spark/jars-ext:/opt/bitnami/spark/delta-jars

  python:
    image: openjdk:17-jdk-slim
    container_name: python
    volumes:
      - ./shared-data:/app
      - ./python-scripts:/scripts
      - ./spark/jars-ext:/opt/delta-jars:ro
    working_dir: /app
    environment:
      - JAVA_HOME=/usr/local/openjdk-17
      - PATH=/usr/local/openjdk-17/bin:$PATH
      - SPARK_HOME=/opt/spark
      - PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
    command: >
      sh -c "
        apt-get update && 
        apt-get install -y curl procps python3 python3-pip wget &&
        ln -s /usr/bin/python3 /usr/bin/python &&
        pip3 install --upgrade pip &&
        pip3 install pyspark findspark &&
        wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz &&
        tar -xzf spark-3.5.1-bin-hadoop3.tgz &&
        mv spark-3.5.1-bin-hadoop3 /opt/spark &&
        rm spark-3.5.1-bin-hadoop3.tgz &&
        java -version &&
        python --version &&
        tail -f /dev/null"
    stdin_open: true
    tty: true

  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_acls_enabled=false
      # CONFIGURACIÓN DE PROXY CORREGIDA para Hue:
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_hadoop_proxyuser_root_hosts=*
      - CORE_CONF_hadoop_proxyuser_root_groups=*
      # Agregar también permisos para cualquier usuario:
      - CORE_CONF_hadoop_proxyuser_admin_hosts=*
      - CORE_CONF_hadoop_proxyuser_admin_groups=*
      # YARN Configuration
      - YARN_CONF_yarn_resourcemanager_hostname=hadoop-namenode
      - YARN_CONF_yarn_resourcemanager_bind___host=0.0.0.0
      - YARN_CONF_yarn_nodemanager_bind___host=0.0.0.0
      - YARN_CONF_yarn_timeline___service_hostname=hadoop-namenode
      - YARN_CONF_mapreduce_map_memory_mb=2048
      - YARN_CONF_mapreduce_reduce_memory_mb=4096
      - YARN_CONF_yarn_scheduler_maximum___allocation___mb=8192
      - YARN_CONF_yarn_nodemanager_resource_memory___mb=4096
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"  # NameNode Web UI
      - "9000:9000"  # HDFS port
      - "8088:8088"  # ResourceManager Web UI
      - "19888:19888" # Job History Server
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    depends_on:
      hadoop-namenode:
        condition: service_healthy
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
    volumes:
      - hdfs_datanode:/hadoop/dfs/data
    ports:
      - "9864:9864"  # DataNode Web UI 

  yarn-resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: yarn-resourcemanager
    depends_on:
      hadoop-namenode:
        condition: service_healthy
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=yarn-resourcemanager
      - YARN_CONF_yarn_resourcemanager_bind___host=0.0.0.0
      - YARN_CONF_yarn_timeline___service_hostname=yarn-resourcemanager
      - YARN_CONF_mapreduce_map_memory_mb=2048
      - YARN_CONF_mapreduce_reduce_memory_mb=4096
      - YARN_CONF_yarn_scheduler_maximum___allocation___mb=8192
    ports:
      - "8089:8088"  

  hue:
    image: gethue/hue:latest
    container_name: hue
    ports:
      - "8888:8888"
    environment:
      - HUE_IGNORE_PASSWORD_SCRIPT_ERRORS=True
    volumes:
      - ./shared-data/hue.ini:/usr/share/hue/desktop/conf/hue.ini
      - ./shared-data/hue-data:/hue
    depends_on:
      hadoop-namenode:
        condition: service_healthy
      hadoop-datanode:
        condition: service_started
      yarn-resourcemanager:
        condition: service_started

volumes:
  postgres_data:
  nifi_database_repository:
  nifi_flowfile_repository:
  nifi_content_repository:
  nifi_provenance_repository:
  nifi_state:
  nifi_logs:
  hdfs_namenode:
  hdfs_datanode:
  hue_data: